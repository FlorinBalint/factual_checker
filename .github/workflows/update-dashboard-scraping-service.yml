name: Update Dashboard Data (Scraping Service)

on:
  schedule:
    # Run daily at 6:00 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:  # Allow manual triggering

jobs:
  update-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install playwright
        
    - name: Install Playwright browsers
      run: |
        playwright install chromium
        
    - name: Create Playwright-based crawler
      run: |
        cat > app/playwright_crawler.py << 'EOF'
        from playwright.sync_api import sync_playwright
        import time
        import random
        import logging
        
        logger = logging.getLogger(__name__)
        
        class PlaywrightCrawler:
            def __init__(self):
                self.playwright = None
                self.browser = None
                self.context = None
                
            def __enter__(self):
                self.playwright = sync_playwright().start()
                self.browser = self.playwright.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-blink-features=AutomationControlled',
                        '--disable-web-security',
                        '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                    ]
                )
                self.context = self.browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    locale='ro-RO'
                )
                return self
                
            def __exit__(self, exc_type, exc_val, exc_tb):
                if self.context:
                    self.context.close()
                if self.browser:
                    self.browser.close()
                if self.playwright:
                    self.playwright.stop()
                    
            def get_page_content(self, url):
                try:
                    page = self.context.new_page()
                    
                    # Random delay
                    time.sleep(random.uniform(2, 5))
                    
                    # Navigate with retry logic
                    for attempt in range(3):
                        try:
                            response = page.goto(url, wait_until='networkidle', timeout=30000)
                            if response.status < 400:
                                break
                        except Exception as e:
                            logger.warning(f"Attempt {attempt + 1} failed for {url}: {e}")
                            if attempt == 2:
                                raise
                            time.sleep(random.uniform(3, 7))
                    
                    # Wait for content to load
                    page.wait_for_timeout(2000)
                    
                    content = page.content()
                    page.close()
                    return content
                    
                except Exception as e:
                    logger.error(f"Failed to fetch {url}: {e}")
                    return None
        EOF
        
    - name: Create Playwright-enabled main script
      run: |
        cat > app/playwright_main.py << 'EOF'
        from playwright_crawler import PlaywrightCrawler
        from politician_fact_crawler import PoliticianFactsCrawler
        from politicians_crawler import PoliticiansCrawler
        from pyquery import PyQuery as pq
        import main
        import requests
        import logging
        
        logger = logging.getLogger(__name__)
        
        # Create global crawler instance
        playwright_crawler = None
        
        def init_crawler():
            global playwright_crawler
            if playwright_crawler is None:
                playwright_crawler = PlaywrightCrawler()
                playwright_crawler.__enter__()
        
        def cleanup_crawler():
            global playwright_crawler
            if playwright_crawler is not None:
                playwright_crawler.__exit__(None, None, None)
                playwright_crawler = None
        
        # Monkey patch requests.get to use Playwright
        original_get = requests.get
        def playwright_get(url, **kwargs):
            init_crawler()
            content = playwright_crawler.get_page_content(url)
            if content:
                # Create a mock response object
                class MockResponse:
                    def __init__(self, text, status_code=200):
                        self.text = text
                        self.status_code = status_code
                    def raise_for_status(self):
                        if self.status_code >= 400:
                            raise requests.exceptions.HTTPError()
                return MockResponse(content)
            else:
                raise requests.exceptions.RequestException("Failed to fetch with Playwright")
        
        requests.get = playwright_get
        
        # Run the main script
        if __name__ == "__main__":
            try:
                main.main(main.__parser.parse_args())
            finally:
                cleanup_crawler()
        EOF
        
    - name: Generate CSV data with Playwright
      run: |
        cd app
        python playwright_main.py --output ../dashboard/politician_stats.csv --generate_party_stats false --workers 1
      env:
        PYTHONUNBUFFERED: 1
        
    - name: Fallback to predefined politicians if needed
      run: |
        cd app
        if [ ! -f ../dashboard/politician_stats.csv ] || [ ! -s ../dashboard/politician_stats.csv ]; then
          echo "Playwright crawling failed, using predefined politician list..."
          python playwright_main.py --politicians "Klaus Iohannis,Marcel Ciolacu,Elena Lasconi,George Simion,Diana Iovanovici-Șoșoacă,Nicolae Ciucă,Kelemen Hunor" --output ../dashboard/politician_stats.csv --generate_party_stats false --workers 1
        fi
        
    - name: Check if CSV was generated
      run: |
        if [ ! -f dashboard/politician_stats.csv ]; then
          echo "Error: CSV file was not generated"
          exit 1
        fi
        echo "CSV file generated successfully"
        wc -l dashboard/politician_stats.csv
        
    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add dashboard/politician_stats.csv
        
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "Auto-update politician stats data - $(date '+%Y-%m-%d %H:%M UTC')"
          git push
        fi